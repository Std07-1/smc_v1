"""ĞŸĞ»Ğ°Ğ½ÑƒĞ²Ğ°Ğ»ÑŒĞ½Ğ¸Ğº Ğ¿ĞµÑ€Ñ–Ğ¾Ğ´Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ prefilter Ñ‚Ğ° Ñ–ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ preload.

Ğ¨Ğ»ÑÑ…: ``app/preload_and_update.py``

Ğ¤Ğ¾Ğ½Ğ¾Ğ²Ñ– Ğ·Ğ°Ğ´Ğ°Ñ‡Ñ–:
    â€¢ periodic_prefilter_and_update â€” Ğ¿ĞµÑ€Ñ–Ğ¾Ğ´Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ğ¸ĞºĞ¾Ğ½ÑƒÑ” Stage1 prefilter Ñ– Ğ¾Ğ½Ğ¾Ğ²Ğ»ÑÑ” fast_symbols.
    â€¢ preload_1m_history â€” Ğ¼Ğ°ÑĞ¾Ğ²Ğµ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ–Ñ… 1m Ğ±Ğ°Ñ€Ñ–Ğ² Ğ´Ğ»Ñ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ€Ñ‚Ñƒ RAM ÑˆĞ°Ñ€Ñƒ.
    â€¢ preload_daily_levels â€” Ğ²Ğ¸Ğ±Ñ–Ñ€ĞºĞ° Ğ´ĞµĞ½Ğ½Ğ¸Ñ… Ğ±Ğ°Ñ€Ñ–Ğ² Ğ´Ğ»Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ñ€Ñ–Ğ²Ğ½Ñ–Ğ² / ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ğº.
"""

import asyncio
import logging
import time
import uuid
from collections.abc import Iterable
from typing import Any, cast

import aiohttp
import pandas as pd
from rich.console import Console
from rich.logging import RichHandler

from config.config import (
    MANUAL_FAST_SYMBOLS_SEED,
    PREFILTER_INTERVAL_SEC,
    PRELOAD_1M_LOOKBACK_INIT,
    SCREENING_LOOKBACK,
)
from data.unified_store import UnifiedDataStore
from stage1.optimized_asset_filter import get_filtered_assets

# ĞÑ–ÑĞºĞ¾Ñ— Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ— Ñ‡Ğ°ÑÑƒ: Ğ¿Ñ€Ğ°Ñ†ÑÑ”Ğ¼Ğ¾ Ñ–Ğ· ÑĞ¸Ñ€Ğ¸Ğ¼Ğ¸ timestamp Ğ· Binance ÑĞº Ñ”

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ›Ğ¾Ğ³ÑƒĞ²Ğ°Ğ½Ğ½Ñ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logger = logging.getLogger("app.preload")
if not logger.handlers:
    logger.setLevel(logging.INFO)
    # show_path=True Ğ´Ğ»Ñ Ñ‡Ñ–Ñ‚ĞºĞ¾Ñ— Ğ²ĞºĞ°Ğ·Ñ–Ğ²ĞºĞ¸ Ñ„Ğ°Ğ¹Ğ»Ñƒ/Ñ€ÑĞ´ĞºĞ° Ñƒ WARNING/ERROR
    logger.addHandler(RichHandler(console=Console(stderr=True), show_path=True))
    logger.propagate = False


def merge_prefilter_with_manual(
    filtered: Iterable[str],
    manual_seed: Iterable[str],
    manual_overrides: Iterable[str],
) -> tuple[list[str], set[str]]:
    """ĞĞ±'Ñ”Ğ´Ğ½ÑƒÑ” Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸ Ğ¿Ñ€ĞµÑ„Ñ–Ğ»ÑŒÑ‚Ñ€Ğ° Ğ· Ñ€ÑƒÑ‡Ğ½Ğ¸Ğ¼ whitelist.

    Args:
        filtered: ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸, Ñ‰Ğ¾ Ğ¿Ñ€Ğ¾Ğ¹ÑˆĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¹ Ñ„Ñ–Ğ»ÑŒÑ‚Ñ€.
        manual_seed: Ğ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºĞ¾Ğ²Ğ¸Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ–Ğ· ĞºĞ¾Ğ½Ñ„Ñ–Ğ³Ñƒ.
        manual_overrides: ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ†ÑŒĞºÑ– Ñ€ÑƒÑ‡Ğ½Ñ– ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸ (Redis).

    Returns:
        ĞšĞ¾Ñ€Ñ‚ĞµĞ¶ (Ğ¿Ğ¾Ğ²Ğ½Ğ¸Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñƒ Ğ½Ğ¸Ğ¶Ğ½ÑŒĞ¾Ğ¼Ñƒ Ñ€ĞµĞ³Ñ–ÑÑ‚Ñ€Ñ–, Ğ½Ğ°Ğ±Ñ–Ñ€ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ²,
        Ğ´Ğ¾Ğ´Ğ°Ğ½Ğ¸Ñ… ÑĞ°Ğ¼Ğµ Ñ€ÑƒÑ‡Ğ½Ğ¸Ğ¼Ğ¸ ÑĞ¿Ğ¸ÑĞºĞ°Ğ¼Ğ¸).
    """

    filtered_set = {str(sym).lower() for sym in filtered if sym}
    manual_cfg_set = {str(sym).lower() for sym in manual_seed if sym}
    manual_override_set = {str(sym).lower() for sym in manual_overrides if sym}
    combined = filtered_set | manual_cfg_set | manual_override_set
    manual_added = combined - filtered_set
    return sorted(combined), manual_added


# â”€â”€ Ğ¤Ğ¾Ğ½Ğ¾Ğ²Ğ¸Ğ¹ Ñ‚Ğ°ÑĞº: Ğ¿ĞµÑ€Ñ–Ğ¾Ğ´Ğ¸Ñ‡Ğ½Ğµ Ğ¾Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ fast_symbols Ñ‡ĞµÑ€ĞµĞ· prefilter â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def periodic_prefilter_and_update(
    cache,
    session: aiohttp.ClientSession,
    thresholds,
    interval: int = PREFILTER_INTERVAL_SEC,
    buffer: UnifiedDataStore | None = None,
    lookback: int = PRELOAD_1M_LOOKBACK_INIT,
):
    """
    ĞŸĞµÑ€Ñ–Ğ¾Ğ´Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ğ¸ĞºĞ¾Ğ½ÑƒÑ” prefilter Ñ‚Ğ° Ğ¾Ğ½Ğ¾Ğ²Ğ»ÑÑ” fast_symbols Ñƒ Redis.
    Ğ”Ğ¾Ğ´Ğ°Ñ” preload Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ— Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¸Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ñ–Ğ².
    """
    # ĞŸĞ¾Ñ‡Ğ°Ñ‚ĞºĞ¾Ğ²Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ñ–Ñ€ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ²
    manual_seed_cfg = MANUAL_FAST_SYMBOLS_SEED
    try:
        manual_overrides = await cache.get_manual_fast_symbols()
    except AttributeError:
        manual_overrides = []

    initial_combined, manual_added_initial = merge_prefilter_with_manual(
        await cache.get_fast_symbols(), manual_seed_cfg, manual_overrides
    )
    initial_symbols = set(initial_combined)
    prev_symbols = initial_symbols.copy()
    if manual_added_initial:
        logger.info(
            "Prefilter bootstrap: Ğ´Ğ¾Ğ´Ğ°Ğ½Ğ¾ Ñ€ÑƒÑ‡Ğ½Ñ– ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸",
            extra={
                "added": sorted(list(manual_added_initial))[:6],
                "count": len(manual_added_initial),
            },
        )
    if initial_symbols:
        try:
            await cache.set_fast_symbols(
                sorted(initial_symbols), ttl=max(interval * 2, interval + 60)
            )
            logger.debug(
                "Prefilter bootstrap: Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ²Ğ¶ĞµĞ½Ğ¾ TTL Ñ–ÑĞ½ÑƒÑÑ‡Ğ¾Ğ³Ğ¾ whitelist",
                extra={
                    "count": len(initial_symbols),
                    "head": sorted(list(initial_symbols))[:3],
                },
            )
        except Exception as exc:
            logger.warning("ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ¾Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğ¸ TTL Ğ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ whitelist: %s", exc)

    # Ğ—Ğ°Ñ‚Ñ€Ğ¸Ğ¼ĞºĞ° Ğ¿ĞµÑ€ĞµĞ´ Ğ¿ĞµÑ€ÑˆĞ¸Ğ¼ Ğ¾Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑĞ¼ (Ñ‰Ğ¾Ğ± ÑƒĞ½Ğ¸ĞºĞ½ÑƒÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ñ–ĞºÑ‚Ñƒ Ğ· Ğ¿ĞµÑ€Ğ²Ğ¸Ğ½Ğ½Ğ¸Ğ¼ Ğ¿Ñ€ĞµÑ„Ñ–Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ¼)
    await asyncio.sleep(interval)  # Ğ§ĞµĞºĞ°Ñ”Ğ¼Ğ¾ Ğ·Ğ²Ğ¸Ñ‡Ğ°Ğ¹Ğ½Ğ¸Ğ¹ Ñ–Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ» (600 ÑĞµĞº)
    while True:
        batch_id = uuid.uuid4().hex[:8]
        t0 = time.perf_counter()
        try:
            logger.info(
                "ğŸ”„ Ğ¡Ñ‚Ğ°Ñ€Ñ‚ prefilter-Ñ†Ğ¸ĞºĞ»Ñƒ",
                extra={
                    "batch_id": batch_id,
                    "interval_sec": interval,
                    "prev_symbols_count": len(prev_symbols),
                    "prev_head": sorted(list(prev_symbols))[:3],
                    "prev_tail": sorted(list(prev_symbols))[-3:],
                },
            )

            fast_symbols = await get_filtered_assets(
                session=session,
                cache_handler=cache,
                thresholds=thresholds,
                dynamic=True,
            )

            try:
                manual_overrides = await cache.get_manual_fast_symbols()
            except AttributeError:
                manual_overrides = []

            combined_list, manual_added = merge_prefilter_with_manual(
                fast_symbols or [], manual_seed_cfg, manual_overrides
            )

            if combined_list:
                fast_symbols = combined_list
                current_symbols = set(fast_symbols)

                fast_symbols = sorted(current_symbols)
                await cache.set_fast_symbols(
                    fast_symbols, ttl=interval * 2
                )  # TTL 1200 ÑĞµĞº

                added = sorted(list(current_symbols - prev_symbols))
                removed = sorted(list(prev_symbols - current_symbols))
                logger.info(
                    "Prefilter: Ğ¾Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾ fast_symbols",
                    extra={
                        "batch_id": batch_id,
                        "count": len(fast_symbols),
                        "head": fast_symbols[:3],
                        "tail": fast_symbols[-3:],
                        "added_count": len(added),
                        "removed_count": len(removed),
                        "added_head": added[:3],
                        "added_tail": added[-3:],
                        "removed_head": removed[:3],
                        "removed_tail": removed[-3:],
                        "manual_added": sorted(list(manual_added))[:6],
                    },
                )

                # â”€â”€ preload Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¸Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ñ–Ğ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                if buffer is not None:
                    # Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ¢Ğ†Ğ›Ğ¬ĞšĞ˜ Ğ½Ğ¾Ğ²Ñ– ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸
                    new_symbols = current_symbols - prev_symbols

                    # Ğ”Ğ¾Ğ´Ğ°Ñ”Ğ¼Ğ¾ debug-Ğ»Ğ¾Ğ³ Ğ´Ğ»Ñ Ğ²Ñ–Ğ´ÑÑ‚ĞµĞ¶ĞµĞ½Ğ½Ñ ÑÑ‚Ğ°Ğ½Ñ–Ğ² ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ²
                    logger.debug(
                        "Ğ¡Ñ‚Ğ°Ğ½ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ²",
                        extra={
                            "batch_id": batch_id,
                            "current": len(current_symbols),
                            "previous": len(prev_symbols),
                            "new": len(new_symbols),
                            "new_head": sorted(list(new_symbols))[:3],
                            "new_tail": sorted(list(new_symbols))[-3:],
                        },
                    )
                    if new_symbols:
                        new_symbols_list = sorted(list(new_symbols))
                        logger.info(
                            "Preload Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ— Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¸Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ñ–Ğ²",
                            extra={
                                "batch_id": batch_id,
                                "count": len(new_symbols_list),
                                "head": new_symbols_list[:3],
                                "tail": new_symbols_list[-3:],
                                "lookback": lookback,
                            },
                        )
                        await preload_1m_history(
                            new_symbols_list, buffer, lookback=lookback, session=session
                        )

                # ĞĞ½Ğ¾Ğ²Ğ»ÑÑ”Ğ¼Ğ¾ Ğ¿Ğ¾Ğ¿ĞµÑ€ĞµĞ´Ğ½Ñ– ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸
                prev_symbols = current_symbols
            else:
                manual_added = set()
                if prev_symbols:
                    logger.warning(
                        "Prefilter Ğ¿Ğ¾Ğ²ĞµÑ€Ğ½ÑƒĞ² Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ–Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº, Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ”Ğ¼Ğ¾ Ğ¿Ğ¾Ğ¿ĞµÑ€ĞµĞ´Ğ½Ñ–Ğ¹ whitelist",
                        extra={
                            "batch_id": batch_id,
                            "count": len(prev_symbols),
                        },
                    )
                    await cache.set_fast_symbols(
                        sorted(prev_symbols), ttl=max(interval, interval // 2)
                    )
                else:
                    logger.debug(
                        "Prefilter Ğ¿Ğ¾Ğ²ĞµÑ€Ğ½ÑƒĞ² Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ–Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº, Ğ° Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ— whitelist Ğ½ĞµĞ¼Ğ°Ñ”.",
                        extra={"batch_id": batch_id},
                    )
        except Exception as e:
            logger.warning(
                "ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¾Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ prefilter",
                extra={"batch_id": batch_id, "error": str(e)},
            )
            if prev_symbols:
                try:
                    await cache.set_fast_symbols(
                        sorted(prev_symbols), ttl=max(interval, interval // 2)
                    )
                except Exception as ttl_exc:
                    logger.warning(
                        "ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ñ‚Ğ¸ TTL whitelist Ğ¿Ñ–ÑĞ»Ñ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸ prefilter: %s",
                        ttl_exc,
                    )
        finally:
            t1 = time.perf_counter()
            logger.info(
                "âœ… Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ prefilter-Ñ†Ğ¸ĞºĞ»Ñƒ",
                extra={"batch_id": batch_id, "duration_sec": round(t1 - t0, 3)},
            )

        await asyncio.sleep(interval)  # 600 ÑĞµĞº


# â”€â”€ Preload Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ— Ğ´Ğ»Ñ Stage1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def _fetch_batch(
    symbols: list[str], interval: str, limit: int, session: aiohttp.ClientSession
) -> dict[str, pd.DataFrame]:
    """ĞŸĞ°ĞºĞµÑ‚Ğ½Ğµ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ´Ğ°Ğ½Ğ¸Ñ… Ğ´Ğ»Ñ Ğ³Ñ€ÑƒĞ¿Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ² Ğ· Ğ¾Ğ±Ğ¼ĞµĞ¶ĞµĞ½Ğ½ÑĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»ĞµĞ»Ñ–Ğ·Ğ¼Ñƒ."""
    if not symbols:
        return {}

    batch_id = uuid.uuid4().hex[:8]
    t0 = time.perf_counter()
    logger.info(
        "Ğ¡Ñ‚Ğ°Ñ€Ñ‚ Ğ¿Ğ°ĞºĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ",
        extra={
            "batch_id": batch_id,
            "symbols": len(symbols),
            "interval": interval,
            "limit": limit,
            "head": symbols[:3],
            "tail": symbols[-3:],
        },
    )

    semaphore = asyncio.Semaphore(5)  # ĞĞ±Ğ¼ĞµĞ¶ĞµĞ½Ğ½Ñ Ğ´Ğ»Ñ Ğ´ĞµĞ½Ğ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ¸Ñ…
    results: dict[str, pd.DataFrame] = {}

    async def fetch_single(symbol: str):
        async with semaphore:
            try:
                df = await _fetch_klines(symbol, interval, limit, session)
                if df is not None and not df.empty:
                    results[symbol] = df
                    logger.debug(
                        "âœ… ĞŸĞ°ĞºĞµÑ‚Ğ½Ğµ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ: Ğ´Ğ°Ğ½Ñ– Ğ¾Ñ‚Ñ€Ğ¸Ğ¼Ğ°Ğ½Ğ¾",
                        extra={
                            "batch_id": batch_id,
                            "symbol": symbol,
                            "rows": len(df),
                        },
                    )
                else:
                    results[symbol] = pd.DataFrame()
                    logger.warning(
                        "âŒ ĞŸĞ°ĞºĞµÑ‚Ğ½Ğµ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ: Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ– Ğ´Ğ°Ğ½Ñ–",
                        extra={"batch_id": batch_id, "symbol": symbol},
                    )
            except Exception as e:
                results[symbol] = pd.DataFrame()
                logger.warning(
                    "ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ğ°ĞºĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ",
                    extra={"batch_id": batch_id, "symbol": symbol, "error": str(e)},
                )
            await asyncio.sleep(0.1)  # ĞĞµĞ²ĞµĞ»Ğ¸ĞºĞ° Ğ·Ğ°Ñ‚Ñ€Ğ¸Ğ¼ĞºĞ° Ğ¼Ñ–Ğ¶ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ°Ğ¼Ğ¸

    tasks = [fetch_single(symbol) for symbol in symbols]
    await asyncio.gather(*tasks, return_exceptions=True)

    ok = sum(1 for df in results.values() if not df.empty)
    t1 = time.perf_counter()
    logger.info(
        "Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾ Ğ¿Ğ°ĞºĞµÑ‚Ğ½Ğµ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ",
        extra={
            "batch_id": batch_id,
            "ok": ok,
            "total": len(symbols),
            "duration_sec": round(t1 - t0, 3),
        },
    )

    return results


async def _fetch_klines(
    symbol: str,
    interval: str,
    limit: int,
    session: aiohttp.ClientSession,
    start_time: int | None = None,
    end_time: int | None = None,
) -> pd.DataFrame | None:
    """ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğµ Ğ¾Ñ‚Ñ€Ğ¸Ğ¼Ğ°Ğ½Ğ½Ñ klines Ğ´Ğ°Ğ½Ğ¸Ñ… Ğ· Binance REST API."""
    params: dict[str, str | int] = {
        "symbol": symbol.upper(),
        "interval": interval,
        "limit": min(int(limit), 1000),
    }

    if start_time is not None:
        params["startTime"] = int(start_time)
    if end_time is not None:
        params["endTime"] = int(end_time)

    url = "https://api.binance.com/api/v3/klines"

    try:
        async with session.get(
            url, params=params, timeout=aiohttp.ClientTimeout(total=10)
        ) as response:
            if response.status == 200:
                data = await response.json()

                if not data:
                    logger.warning(
                        "ĞŸĞ¾Ñ€Ğ¾Ğ¶Ğ½Ñ Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´ÑŒ Ğ²Ñ–Ğ´ Binance", extra={"symbol": symbol}
                    )
                    return pd.DataFrame()

                df = pd.DataFrame(
                    data,
                    columns=[
                        "open_time",
                        "open",
                        "high",
                        "low",
                        "close",
                        "volume",
                        "close_time",
                        "quote_asset_volume",
                        "trades",
                        "taker_buy_base",
                        "taker_buy_quote",
                        "ignore",
                    ],
                )

                # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ñ–Ñ Ñ‚Ğ¸Ğ¿Ñ–Ğ² Ğ´Ğ°Ğ½Ğ¸Ñ…
                numeric_cols = [
                    "open",
                    "high",
                    "low",
                    "close",
                    "volume",
                    "quote_asset_volume",
                ]
                for col in numeric_cols:
                    df[col] = pd.to_numeric(df[col], errors="coerce")

                df["open_time"] = pd.to_numeric(df["open_time"])
                df["close_time"] = pd.to_numeric(df["close_time"])
                df["trades"] = pd.to_numeric(df["trades"])

                # Ğ›Ğ¾Ğ³ÑƒĞ²Ğ°Ğ½Ğ½Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ñƒ
                if logger.isEnabledFor(logging.DEBUG):
                    try:
                        logger.debug(
                            "ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ DataFrame",
                            extra={
                                "symbol": symbol,
                                "cols": list(map(str, df.columns.tolist())),
                            },
                        )
                    except Exception:
                        pass

                # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ĞºĞ° Ñ†Ñ–Ğ»Ñ–ÑĞ½Ğ¾ÑÑ‚Ñ– Ñ‡Ğ°ÑĞ¾Ğ²Ğ¸Ñ… Ğ¼Ñ–Ñ‚Ğ¾Ğº
                if len(df) > 1:
                    open_time_series = pd.to_numeric(df["open_time"], errors="coerce")
                    time_diff = open_time_series.diff().iloc[1:].astype("float64")
                    expected_interval = float(_get_interval_ms(interval))
                    deviation = (time_diff - expected_interval).abs()
                    anomalies = time_diff[deviation > 1000.0]  # Ğ”Ğ¾Ğ¿ÑƒÑĞº 1 ÑĞµĞºÑƒĞ½Ğ´Ğ°

                    if not anomalies.empty:
                        logger.warning(
                            "ĞĞ½Ğ¾Ğ¼Ğ°Ğ»Ñ–Ñ— Ñ‡Ğ°ÑÑƒ",
                            extra={
                                "symbol": symbol,
                                "interval": interval,
                                "first": anomalies.head(3).astype(int).tolist(),
                            },
                        )

                # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ Ğ»Ğ¾Ğ³ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ´Ğ»Ñ DEBUG
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        "ĞÑ‚Ñ€Ğ¸Ğ¼Ğ°Ğ½Ğ¾ klines",
                        extra={
                            "symbol": symbol,
                            "interval": interval,
                            "rows": len(df),
                            "ts_head": df["open_time"].head(3).astype(int).tolist(),
                            "ts_tail": df["open_time"].tail(3).astype(int).tolist(),
                        },
                    )

                return df
            else:
                txt = await response.text()
                logger.debug(
                    "HTTP Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ²Ñ–Ğ´ Binance",
                    extra={
                        "symbol": symbol,
                        "interval": interval,
                        "status": response.status,
                        "body_head": txt[:200],
                    },
                )
                return None

    except TimeoutError:
        logger.error("Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñƒ", extra={"symbol": symbol, "interval": interval})
        return None
    except Exception as e:
        logger.error(
            "ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ–Ğ´ Ñ‡Ğ°Ñ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñƒ",
            extra={"symbol": symbol, "interval": interval, "error": str(e)},
        )
        return None


def _get_interval_ms(interval: str) -> int:
    """ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚ÑƒÑ” Ñ–Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ» Ñƒ Ğ¼Ñ–Ğ»Ñ–ÑĞµĞºÑƒĞ½Ğ´Ğ¸."""
    intervals = {
        "1m": 60000,
        "3m": 180000,
        "5m": 300000,
        "15m": 900000,
        "30m": 1800000,
        "1h": 3600000,
        "2h": 7200000,
        "4h": 14400000,
        "6h": 21600000,
        "8h": 28800000,
        "12h": 43200000,
        "1d": 86400000,
    }
    return intervals.get(interval, 60000)


async def preload_1m_history(
    fast_symbols: list[str],
    store: UnifiedDataStore,
    lookback: int = SCREENING_LOOKBACK,
    session: aiohttp.ClientSession | None = None,
) -> dict:
    """ĞœĞ°ÑĞ¾Ğ²Ğµ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ 1Ñ… Ñ…Ğ²Ğ¸Ğ»Ğ¸Ğ½Ğ½Ğ¾Ñ— Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ— Ğ´Ğ»Ñ ÑĞ¿Ğ¸ÑĞºÑƒ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ².

    Args:
        fast_symbols: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ
        store: UnifiedDataStore (Ğ½Ğµ ÑĞ»Ğ¾Ğ²Ğ½Ğ¸Ğº!)
        lookback: Ğ“Ğ»Ğ¸Ğ±Ğ¸Ğ½Ğ° Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ— Ğ² Ğ±Ğ°Ñ€Ğ°Ñ…
        session: AIOHTTP ÑĞµÑÑ–Ñ (ÑÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñƒ ÑĞºÑ‰Ğ¾ None)

    Returns:
        Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ
    """
    if not fast_symbols:
        logger.warning("Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ² Ğ´Ğ»Ñ preload Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ–Ğ¹")
        return {"total": 0, "success": 0, "failed": 0, "duration": 0}

    close_session = False
    if session is None:
        session = aiohttp.ClientSession()
        close_session = True

    stats: dict[str, Any] = {
        "total": len(fast_symbols),
        "success": 0,
        "failed": 0,
        "start_time": time.time(),
        "symbols_loaded": [],
    }

    try:
        semaphore = asyncio.Semaphore(10)

        async def fetch_with_semaphore(symbol):
            async with semaphore:
                df = await _fetch_klines(symbol, "1m", lookback, session)
                if df is not None and not df.empty:
                    # Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ”Ğ¼Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¸Ğ¹ API UnifiedDataStore
                    await store.put_bars(symbol, "1m", df)
                    stats["success"] = cast(int, stats.get("success", 0)) + 1
                    stats["symbols_loaded"].append(symbol)

                    logger.debug(
                        f"âœ… {symbol}: {len(df)} Ğ±Ğ°Ñ€Ñ–Ğ² | "
                        f"ĞÑÑ‚Ğ°Ğ½Ğ½Ñ–Ğ¹: {pd.to_datetime(df['open_time'].iloc[-1], unit='ms').strftime('%H:%M:%S')}"
                    )
                    return True
                else:
                    stats["failed"] = cast(int, stats.get("failed", 0)) + 1
                    logger.warning(f"âŒ {symbol}: Ğ½Ğµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸")
                    return False

        tasks = [fetch_with_semaphore(symbol) for symbol in fast_symbols]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                stats["failed"] = cast(int, stats.get("failed", 0)) + 1
                logger.error(f"ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ñƒ {fast_symbols[i]}: {result}")

    finally:
        if close_session:
            await session.close()

    end_time = time.time()
    start_time_val = cast(float, stats.get("start_time", end_time))
    stats["end_time"] = end_time
    stats["duration"] = float(end_time - start_time_val)

    success = cast(int, stats.get("success", 0))
    total = cast(int, stats.get("total", 0))
    success_rate = (success / total * 100) if total > 0 else 0.0
    logger.info(
        f"ğŸ“Š Preload 1m: {stats['success']}/{stats['total']} "
        f"({success_rate:.1f}%) | Ğ§Ğ°Ñ: {stats['duration']:.2f}Ñ"
    )

    return stats
