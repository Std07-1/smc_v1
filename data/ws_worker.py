"""Binance Futures WebSocket –≤–æ—Ä–∫–µ—Ä ‚Üí UnifiedDataStore / Redis.

–®–ª—è—Ö: ``data/ws_worker.py``

–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è:
    ‚Ä¢ –æ—Ç—Ä–∏–º—É—î –ø–æ—Ç—ñ–∫ 1m kline —á–µ—Ä–µ–∑ Binance Futures WS;
    ‚Ä¢ –æ–Ω–æ–≤–ª—é—î —Ö–≤–∏–ª–∏–Ω–Ω—É —ñ—Å—Ç–æ—Ä—ñ—é (legacy blob –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –∑—á–∏—Ç—É–≤–∞–Ω–Ω—è + UnifiedDataStore.put_bars);
    ‚Ä¢ –ø—Ä–∏ —Ñ—ñ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—ó —Ö–≤–∏–ª–∏–Ω–∏ –∞–≥—Ä–µ–≥—É—î 1h –±–∞—Ä —ñ –ø—É–±–ª—ñ–∫—É—î –æ–Ω–æ–≤–ª–µ–Ω–Ω—è (``klines.1h.update``);
    ‚Ä¢ –ø—ñ–¥—Ç—Ä–∏–º—É—î –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π whitelist —Å–∏–º–≤–æ–ª—ñ–≤ (prefilter —á–µ—Ä–µ–∑ Redis —Å–µ–ª–µ–∫—Ç–æ—Ä).

–û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ:
    ‚Ä¢ reconnect —ñ–∑ –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–∏–º backoff;
    ‚Ä¢ –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–∏–π refresh —Å–∏–º–≤–æ–ª—ñ–≤ –±–µ–∑ –ø–æ–≤–Ω–æ–≥–æ reconnect (UNSUBSCRIBE/SUBSCRIBE);
    ‚Ä¢ fallback –Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω—ñ —Å–∏–º–≤–æ–ª–∏, —è–∫—â–æ —Å–µ–ª–µ–∫—Ç–æ—Ä –ø–æ—Ä–æ–∂–Ω—ñ–π —á–∏ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π.
"""

import asyncio
import json
import logging
import os
import time
from typing import Any, cast

import aiohttp
import orjson
import pandas as pd
import websockets
from lz4.frame import compress, decompress
from rich.console import Console
from rich.logging import RichHandler

# unified store (single source of truth)
from config.config import WS_GAP_BACKFILL, WS_GAP_STATUS_PATH
from data.unified_store import UnifiedDataStore

# –ê—É–¥–∏—Ç: –∂–æ–¥–Ω–æ—ó –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó —á–∞—Å—É ‚Äî –ø—Ä–∞—Ü—é—î–º–æ —ñ–∑ —Å–∏—Ä–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ —è–∫ –ø—Ä–∏—Ö–æ–¥—è—Ç—å


# ‚îÄ‚îÄ –í–±—É–¥–æ–≤–∞–Ω—ñ (–º—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ) —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ç–æ—Ä–∏ DataFrame (–≤–∏–¥–∞–ª–µ–Ω–æ raw_data.py) ‚îÄ‚îÄ
def _df_to_bytes(df: pd.DataFrame, *, compress_lz4: bool = True) -> bytes:
    """–°–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è DataFrame ‚Üí bytes (orjson + –æ–ø—Ü—ñ–π–Ω–æ LZ4).

    –¢—ñ–ª—å–∫–∏ –∫–æ–ª–æ–Ω–∫–∏ —Ç–∞ —ñ–Ω–¥–µ–∫—Å: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ñ–æ—Ä–º–∞—Ç orient="split".
    –Ø–∫—â–æ —î datetime –∫–æ–ª–æ–Ω–∫–∞ `timestamp`, –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —É int64 ms –¥–ª—è JS.
    """
    df_out = df.copy()
    # 1) –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ datetime-–∫–æ–ª–æ–Ω–∫—É 'timestamp' —É –º—Å (int64) –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ JS
    if "timestamp" in df_out.columns and pd.api.types.is_datetime64_any_dtype(
        df_out["timestamp"]
    ):
        df_out["timestamp"] = (df_out["timestamp"].astype("int64") // 1_000_000).astype(
            "int64"
        )
    # 2) –Ø–∫—â–æ —ñ–Ω–¥–µ–∫—Å datetime-–ø–æ–¥—ñ–±–Ω–∏–π ‚Äî —Ç–µ–∂ –∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —É –º—Å (int64),
    #    —ñ–Ω–∞–∫—à–µ orjson –Ω–µ –∑–º–æ–∂–µ —Å–µ—Ä—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ pandas.Timestamp —É split['index']
    try:
        if isinstance(
            df_out.index, pd.DatetimeIndex
        ) or pd.api.types.is_datetime64_any_dtype(df_out.index):
            idx_ms = (df_out.index.view("int64") // 1_000_000).astype("int64")
            df_out.index = idx_ms
    except Exception:
        # —ñ–Ω–¥–µ–∫—Å –Ω–µ datetime –∞–±–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è ‚Äî –∑–∞–ª–∏—à–∞—î–º–æ —è–∫ —î
        pass
    raw_json = orjson.dumps(df_out.to_dict(orient="split"))
    if compress_lz4:
        return cast(bytes, compress(raw_json))
    # raw_json is bytes from orjson.dumps
    return raw_json


def _bytes_to_df(buf: bytes | str, *, compressed: bool = True) -> pd.DataFrame:
    """–î–µ—Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è bytes ‚Üí DataFrame (reverse _df_to_bytes)."""
    raw = buf.encode() if isinstance(buf, str) else buf
    if compressed:
        raw = decompress(raw)
    obj = orjson.loads(raw)
    df = pd.DataFrame(**obj)
    # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ datetime-—ñ–Ω–¥–µ–∫—Å, —è–∫—â–æ —ñ–Ω–¥–µ–∫—Å —Ü—ñ–ª–∏–π (–º—Å)
    try:
        if getattr(
            df.index, "dtype", None
        ) is not None and pd.api.types.is_integer_dtype(df.index):
            df.index = pd.to_datetime(df.index.astype("int64"), unit="ms", utc=True)
    except Exception:
        pass
    # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ –∫–æ–ª–æ–Ω–∫—É 'timestamp', —è–∫—â–æ –≤–æ–Ω–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —è–∫ —Ü—ñ–ª—ñ —á–∏—Å–ª–∞ (–º—Å)
    if "timestamp" in df.columns and pd.api.types.is_integer_dtype(df["timestamp"]):
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
    return df


# (moved UnifiedDataStore import to top for ruff E402 compliance)

# ‚îÄ‚îÄ –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è / –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PARTIAL_CHANNEL = "klines.1m.partial"
FINAL_CHANNEL = "klines.1h.update"
# Default TTLs for legacy blob snapshot (NOT the canonical ds.cfg.intervals_ttl)
DEFAULT_INTERVALS_TTL: dict[str, int] = {"1m": 90, "1h": 65 * 60}
SELECTOR_REFRESH_S = 30  # how often to refresh symbol whitelist

STATIC_SYMBOLS = os.getenv("STREAM_SYMBOLS", "")
DEFAULT_SYMBOLS = [s.lower() for s in STATIC_SYMBOLS.split(",") if s] or ["btcusdt"]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –õ–æ–≥—É–≤–∞–Ω–Ω—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logger = logging.getLogger("app.data.ws_worker")
if not logger.handlers:
    logger.setLevel(logging.INFO)
    # show_path=True –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –º—ñ—Å—Ü—è –ø–æ—Ö–æ–¥–∂–µ–Ω–Ω—è WARNING/ERROR
    logger.addHandler(RichHandler(console=Console(stderr=True), show_path=True))
    logger.propagate = False

logging.getLogger("websockets").setLevel(logging.WARNING)
logging.getLogger("websockets.client.protocol").setLevel(logging.WARNING)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ WS Worker ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


class WSWorker:
    """WebSocket worker streaming 1m Binance klines directly into UnifiedDataStore.

    Legacy SimpleCacheHandler / RAMBuffer are removed. Minute bars persisted via
    store.put_bars; last bar & fast symbols served from Redis through the store.
    """

    def __init__(
        self,
        symbols: list[str] | None = None,
        *,
        store: UnifiedDataStore,
        selectors_key: str | None = None,
        intervals_ttl: dict[str, int] | None = None,
    ):
        if store is None:
            raise ValueError("WSWorker requires a UnifiedDataStore instance")
        self.store = store
        self._symbols: set[str] = set(
            [s.lower() for s in symbols] if symbols else DEFAULT_SYMBOLS
        )
        # optional override for selector redis key ("part1:part2:..")
        self._selectors_key: tuple[str, ...] | None = (
            tuple(p for p in selectors_key.split(":") if p) if selectors_key else None
        )
        # TTLs for legacy blob snapshots; fall back to defaults above
        mapping = DEFAULT_INTERVALS_TTL.copy()
        if intervals_ttl:
            mapping.update(intervals_ttl)
        self._ttl_1m = mapping.get("1m", DEFAULT_INTERVALS_TTL["1m"])
        self._ttl_1h = mapping.get("1h", DEFAULT_INTERVALS_TTL["1h"])
        self._ws_url: str | None = None
        self._backoff: int = 3
        self._refresh_task: asyncio.Task[Any] | None = None
        self._hb_task: asyncio.Task[Any] | None = None
        self._stop_event = asyncio.Event()
        self._resync_state: dict[str, dict[str, Any]] = {}

    async def _get_live_symbols(self) -> list[str]:
        """Fetch whitelist symbols either via custom selectors_key or store helper.

        selectors_key (if provided) is a colon-delimited path inside the store namespace
        e.g. "selectors:fast_symbols" -> ai_one:selectors:fast_symbols
        """
        if self._selectors_key:
            try:
                data = await self.store.redis.jget(*self._selectors_key, default=[])
            except Exception as e:  # pragma: no cover
                logger.warning("[WSWorker] selectors_key fetch failed: %s", e)
                data = []
        else:
            data = await self.store.get_fast_symbols()
        # –î–µ—Ñ–æ–ª—Ç ‚Äî —è–∫—â–æ –Ω—ñ—á–æ–≥–æ –Ω–µ –ø—Ä–∏–π—à–ª–æ, –∞–±–æ —Ç–∏–ø –Ω–µ–≤—ñ–¥–æ–º–∏–π
        syms = []
        if isinstance(data, dict):
            syms = list(data.keys())
        elif isinstance(data, list):
            syms = data
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–æ –Ω–∏–∂–Ω—å–æ–≥–æ —Ä–µ–≥—ñ—Å—Ç—Ä—É –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ Binance streams
        syms = [s.lower() for s in syms]
        # Fallback —è–∫—â–æ –ø–æ—Ä–æ–∂–Ω—ñ–π
        if not syms:
            if self._symbols:
                logger.warning(
                    "[WSWorker] selector:active:stream –ø–æ—Ä–æ–∂–Ω—ñ–π, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π whitelist"
                )
                syms = sorted(self._symbols)
            else:
                logger.warning(
                    "[WSWorker] selector:active:stream –ø—É—Å—Ç–∏–π –∞–±–æ –Ω–µ–≤–∞–ª–∏–¥–Ω–∏–π, fallback btcusdt"
                )
                syms = DEFAULT_SYMBOLS
        if len(syms) < 3:
            logger.warning(
                "[WSWorker] –í–ê–ñ–õ–ò–í–û: –ö—ñ–ª—å–∫—ñ—Å—Ç—å symbols —É —Å—Ç—Ä—ñ–º—ñ –ø—ñ–¥–æ–∑—Ä—ñ–ª–æ –º–∞–ª–∞: %d (%s)",
                len(syms),
                syms,
            )
        logger.debug(
            "[WSWorker][_get_live_symbols] data type: %s, value: %s",
            type(data),
            str(data)[:200],
        )
        logger.debug("[WSWorker] –°–∏–º–≤–æ–ª–∏ –¥–ª—è —Å—Ç—Ä—ñ–º—É: %d (%s...)", len(syms), syms[:10])
        return syms

    def _build_ws_url(self, symbols: set[str]) -> str:
        streams = "/".join(f"{s}@kline_1m" for s in sorted(symbols))
        return f"wss://fstream.binance.com/stream?streams={streams}"

    async def _store_minute(self, sym: str, ts: int, k: dict[str, Any]) -> pd.DataFrame:
        """Incrementally update 1m history snapshot blob (legacy format) for quick replay.

        This keeps backward compatibility for any code still reading the old serialized
        frame while the canonical last bar lives in store.redis (jset candles...).
        """
        raw = await self.store.fetch_from_cache(sym, "1m", prefix="candles", raw=True)
        if raw:
            try:
                df = _bytes_to_df(raw)
            except Exception:
                df = pd.DataFrame(columns=["open", "high", "low", "close", "volume"])
        else:
            df = pd.DataFrame(columns=["open", "high", "low", "close", "volume"])
        dt = pd.to_datetime(ts, unit="ms", utc=True)
        df.at[dt, "open"] = float(k["o"])  # set by label to avoid dtype issues
        df.at[dt, "high"] = float(k["h"])
        df.at[dt, "low"] = float(k["l"])
        df.at[dt, "close"] = float(k["c"])
        df.at[dt, "volume"] = float(k["v"])
        await self.store.store_in_cache(
            sym, "1m", _df_to_bytes(df), ttl=self._ttl_1m, prefix="candles", raw=True
        )
        return df

    async def _on_final_candle(self, sym: str, df_1m: pd.DataFrame) -> None:
        """–ê–≥—Ä–µ–≥—É—î 1h-–±–∞—Ä, –∑–±–µ—Ä—ñ–≥–∞—î —É Redis, –ø—É–±–ª—ñ–∫—É—î –ø–æ–¥—ñ—é."""
        df_1h = (
            df_1m.resample("1h", label="right", closed="right")
            .agg(
                {
                    "open": "first",
                    "high": "max",
                    "low": "min",
                    "close": "last",
                    "volume": "sum",
                }
            )
            .dropna()
        )
        await self.store.store_in_cache(
            sym, "1h", _df_to_bytes(df_1h), ttl=self._ttl_1h, prefix="candles", raw=True
        )
        await self.store.redis.r.publish(FINAL_CHANNEL, sym)
        logger.debug("[%s] 1h closed ‚Üí published %s", sym, FINAL_CHANNEL)

    async def _handle_kline(self, k: dict[str, Any]) -> None:
        """
        –û–±—Ä–æ–±–∫–∞ WS kline:
        - –∑–±–µ—Ä—ñ–≥–∞—î 1m –≤ RAMBuffer (–æ–Ω–æ–≤–ª–µ–Ω–Ω—è bar –ø–æ timestamp, –±–µ–∑ –¥—É–±–ª—é–≤–∞–Ω–Ω—è),
        - –ø—Ä–∏ –∑–∞–∫—Ä–∏—Ç—Ç—ñ bar[x] ‚Äî –∞–≥—Ä–µ–≥—É—î —É 1h.
        """
        sym = k["s"].lower()
        ts = int(k["t"])

        if ts < 1_000_000_000_000:  # < 1e12 ‚Üí —Ü–µ —Å–µ–∫—É–Ω–¥–∏
            ts *= 1_000

        tf = "1m"
        bar = {
            "open": float(k["o"]),
            "high": float(k["h"]),
            "low": float(k["l"]),
            "close": float(k["c"]),
            "volume": float(k["v"]),
            "timestamp": ts,  # –º–∞—î –±—É—Ç–∏ ms
        }
        # –ê—É–¥–∏—Ç: –ª–æ–≥ —Å–∏—Ä–∏—Ö –∑–Ω–∞—á–µ–Ω—å —á–∞—Å—É/–±–∞—Ä—É (–ø–µ—Ä—à—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø–æ —Å–∏–º–≤–æ–ª—É)
        try:
            logger.debug(
                "[WS RECEIVE] %s | t=%s x=%s",
                sym,
                k.get("t"),
                k.get("x"),
            )
            logger.debug(
                "[WS RAW] %s t=%s o=%s h=%s l=%s c=%s v=%s x=%s",
                sym,
                k.get("t"),
                k.get("o"),
                k.get("h"),
                k.get("l"),
                k.get("c"),
                k.get("v"),
                k.get("x"),
            )
        except Exception:
            pass

        # Write via UnifiedDataStore (single row DataFrame) ‚Äî –±–µ–∑ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —á–∞—Å—É
        is_closed = bool(k.get("x", False))
        close_time_val = ts + 60_000 - 1
        df_row = pd.DataFrame(
            [
                {
                    "open_time": ts,
                    "open": bar["open"],
                    "high": bar["high"],
                    "low": bar["low"],
                    "close": bar["close"],
                    "volume": bar["volume"],
                    "close_time": close_time_val,
                    "is_closed": is_closed,
                }
            ]
        )
        try:
            logger.debug(
                "[WS PASS] %s | put_bars %s rows=1 open_time=%s close_time=%s",
                sym,
                tf,
                df_row["open_time"].iloc[0],
                df_row["close_time"].iloc[0],
            )
            await self.store.put_bars(sym, tf, df_row)
        except Exception as e:
            logger.warning("Failed to put bars into UnifiedDataStore: %s", e)
        # –õ—ñ—á–∏–ª—å–Ω–∏–∫ –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å WS
        try:
            self.store.metrics.errors.labels(stage="ws_msg").inc()
        except Exception:
            try:
                self.store.metrics.errors.inc()
            except Exception:
                pass

        # –ó–∞–ø–∏—Å —É Redis (—è–∫ fallback —ñ –¥–ª—è stage2+)
        df_1m = await self._store_minute(sym, ts, k)
        await self.store.redis.r.publish(PARTIAL_CHANNEL, sym)
        if is_closed:
            # Gap-detector: –æ—á—ñ–∫—É—î–º–æ, —â–æ —Ü–µ–π open_time –Ω–∞ 60_000 –±—ñ–ª—å—à–∏–π –∑–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –∑–∞–∫—Ä–∏—Ç–∏–π
            try:
                prev = await self.store.get_df(sym, tf, limit=2)
                if prev is not None and len(prev) >= 2:
                    last_two = prev.tail(2)
                    ot_prev = int(last_two["open_time"].iloc[-2])
                    ot_cur = int(last_two["open_time"].iloc[-1])
                    if (ot_cur - ot_prev) != 60_000:
                        logger.warning(
                            "[WS GAP] %s %s gap detected: prev=%s cur=%s delta=%s",
                            sym,
                            tf,
                            ot_prev,
                            ot_cur,
                            ot_cur - ot_prev,
                        )
                        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π auto-heal: backfill –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö —Ö–≤–∏–ª–∏–Ω —á–µ—Ä–µ–∑ REST
                        missing = (ot_cur - ot_prev) // 60_000 - 1
                        try:
                            enabled = bool(WS_GAP_BACKFILL.get("enabled", False))
                            max_minutes = int(WS_GAP_BACKFILL.get("max_minutes", 10))
                        except Exception:
                            enabled = False
                            max_minutes = 10
                        if enabled and missing > 0:
                            # –û–±–º–µ–∂—É—î–º–æ –±–µ–∫—Ñ—ñ–ª –¥–æ max_minutes, —â–æ–± –Ω–µ –±–ª–æ–∫—É–≤–∞—Ç–∏ —Ç–∞ –Ω–µ DDOS-–∏—Ç–∏ REST
                            max_bars = min(missing, max_minutes)
                            start_ot = ot_cur - 60_000 * max_bars
                            await self._mark_resync(
                                sym=sym,
                                start_open_time=start_ot,
                                end_open_time=ot_cur - 60_000,
                                missing=missing,
                                scheduled=max_bars,
                            )
                            # –ó–∞–ø—É—Å–∫–∞—î–º–æ —É —Ñ–æ–Ω—ñ, —â–æ–± –Ω–µ –±–ª–æ–∫—É–≤–∞—Ç–∏ WS-—Ü–∏–∫–ª
                            asyncio.create_task(
                                self._safe_backfill(
                                    sym=sym,
                                    start_open_time=start_ot,
                                    end_open_time=ot_cur - 60_000,
                                    max_bars=max_bars,
                                )
                            )
            except Exception:
                pass
            # –ü—É–±–ª—ñ–∫—É—î–º–æ 1h –ª–∏—à–µ –Ω–∞ –º–µ–∂—ñ –≥–æ–¥–∏–Ω–∏
            if (close_time_val % 3_600_000) == (3_600_000 - 1):
                await self._on_final_candle(sym, df_1m)

    async def _backfill_gap_1m(
        self,
        sym: str,
        *,
        start_open_time: int,
        end_open_time: int,
        max_bars: int,
    ) -> None:
        """Backfill –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö 1m –±–∞—Ä—ñ–≤ —á–µ—Ä–µ–∑ Binance Futures REST.

        Args:
            sym: —Å–∏–º–≤–æ–ª —É lower (–Ω–∞–ø—Ä., "btcusdt").
            start_open_time: –ø–µ—Ä—à–∏–π –ø—Ä–æ–ø—É—â–µ–Ω–∏–π open_time (ms, UTC).
            end_open_time: –æ—Å—Ç–∞–Ω–Ω—ñ–π –ø—Ä–æ–ø—É—â–µ–Ω–∏–π open_time (ms, UTC).
            max_bars: –≤–µ—Ä—Ö–Ω—è –º–µ–∂–∞ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –±–∞—Ä—ñ–≤ (–∑–∞–∑–≤–∏—á–∞–π –Ω–µ–≤–µ–ª–∏–∫–∏–π).
        """
        url = "https://fapi.binance.com/fapi/v1/klines"
        interval = "1m"
        # Binance –¥–æ–∑–≤–æ–ª—è—î limit –¥–æ ~1500; –¥–ª—è –±–µ–∑–ø–µ–∫–∏ –≤—ñ–∑—å–º–µ–º–æ 1000
        remaining = max_bars
        start = start_open_time
        async with aiohttp.ClientSession() as sess:
            while remaining > 0 and start <= end_open_time:
                limit = min(1000, remaining)
                params: dict[str, str | int | float] = {
                    "symbol": sym.upper(),
                    "interval": interval,
                    "startTime": int(start),
                    # endTime —ñ–Ω–∫–æ–ª–∏ –∫–æ—Ä–∏—Å–Ω–∏–π, –∞–ª–µ –º–æ–∂–Ω–∞ –Ω–µ —Å—Ç–∞–≤–∏—Ç–∏, —â–æ–± –±—Ä–∞—Ç–∏ limit –≤—ñ–¥ start
                    "limit": int(limit),
                }
                timeout = aiohttp.ClientTimeout(total=5.0)
                async with sess.get(url, params=params, timeout=timeout) as resp:
                    if resp.status != 200:
                        txt = await resp.text()
                        raise RuntimeError(
                            f"REST backfill HTTP {resp.status}: {txt[:200]}"
                        )
                    data = await resp.json()
                if not data:
                    break
                # –ü–æ–±—É–¥—É—î–º–æ DataFrame –∑ –ø–æ—Ç—Ä—ñ–±–Ω–∏–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
                rows = []
                for it in data:
                    # —Ñ–æ—Ä–º–∞—Ç: [open_time, o, h, l, c, v, close_time, ... , trades, ...]
                    ot = int(it[0])
                    if ot > end_open_time:
                        break
                    rows.append(
                        {
                            "open_time": ot,
                            "open": float(it[1]),
                            "high": float(it[2]),
                            "low": float(it[3]),
                            "close": float(it[4]),
                            "volume": float(it[5]),
                            "close_time": int(it[6]),
                            "is_closed": True,
                        }
                    )
                if not rows:
                    break
                df = pd.DataFrame(rows)
                await self.store.put_bars(sym, "1m", df)
                # –†—É—Ö–∞—î–º–æ—Å—è –¥–∞–ª—ñ
                last_ot = int(df["open_time"].iloc[-1])
                if last_ot >= end_open_time:
                    break
                start = last_ot + 60_000
                remaining -= len(df)
            # –°—Ç–µ–Ω–æ–≥—Ä–∞–º–∞ –≤–∏–¥–∞–ª–µ–Ω–∞
            # –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π reactive Stage1 hook: –≤–∏–∫–ª–∏–∫–∞—Ç–∏ –º–æ–Ω—ñ—Ç–æ—Ä –æ–¥—Ä–∞–∑—É –ø—ñ—Å–ª—è –∑–∞–∫—Ä–∏—Ç—Ç—è –±–∞—Ä—É
            try:
                # prefer centralized config, allow env override
                try:
                    from config.config import REACTIVE_STAGE1

                    reactive = bool(REACTIVE_STAGE1)
                except Exception:
                    reactive = False
                try:
                    import os

                    env_val = os.getenv("REACTIVE_STAGE1", None)
                    if env_val is not None:
                        reactive = env_val in ("1", "true", "True")
                except Exception:
                    pass

                monitor = getattr(self.store, "stage1_monitor", None)
                if reactive and monitor is not None:
                    # –Ω–µ –±–ª–æ–∫—É—î–º–æ WS ‚Äì –∑–∞–ø—É—Å–∫–∞—î–º–æ —è–∫ –æ–∫—Ä–µ–º—É –∑–∞–¥–∞—á—É
                    import asyncio as _asyncio

                    _asyncio.create_task(self._reactive_stage1_call(monitor, sym, None))
            except Exception:  # pragma: no cover
                pass

    async def _safe_backfill(
        self,
        *,
        sym: str,
        start_open_time: int,
        end_open_time: int,
        max_bars: int,
    ) -> None:
        """–ë–µ–∑–ø–µ—á–Ω–∏–π –æ–±–≥–æ—Ä—Ç–Ω–∏–∫ backfill –∑ –ø–µ—Ä–µ—Ö–æ–ø–ª–µ–Ω–Ω—è–º –≤–∏–Ω—è—Ç–∫—ñ–≤."""
        try:
            await self._backfill_gap_1m(
                sym,
                start_open_time=start_open_time,
                end_open_time=end_open_time,
                max_bars=max_bars,
            )
            logger.info(
                "[WS GAP] backfill done %s 1m: %s‚Üí%s (%s bars)",
                sym,
                start_open_time,
                end_open_time,
                max_bars,
            )
            await self._clear_resync(sym)
        except Exception as e:
            logger.warning("[WS GAP] backfill failed %s 1m: %s", sym, e, exc_info=True)

    async def _refresh_symbols(self, ws: Any) -> None:
        """–§–æ–Ω–æ–≤–∏–π —Ç–∞—Å–∫: refresh whitelist —ñ —Ä–µ—Å–∞–±—Å–∫—Ä–∞–π–±."""
        while not self._stop_event.is_set():
            await asyncio.sleep(SELECTOR_REFRESH_S)
            try:
                new_syms = set(await self._get_live_symbols())
                if new_syms and new_syms != self._symbols:
                    await self._resubscribe(ws, new_syms)
            except Exception as e:
                logger.warning("Refresh symbols error: %s", e)

    async def _mark_resync(
        self,
        *,
        sym: str,
        start_open_time: int,
        end_open_time: int,
        missing: int,
        scheduled: int,
    ) -> None:
        meta = {
            "status": "syncing",
            "start_open_time": start_open_time,
            "end_open_time": end_open_time,
            "missing": missing,
            "scheduled": scheduled,
            "ts": int(time.time()),
        }
        prev = self._resync_state.get(sym)
        if prev != meta:
            logger.warning(
                "[WS GAP] %s: –ø–∞—É–∑–∞ Stage1, –ø—Ä–æ–ø—É—â–µ–Ω–æ %d —Ö–≤ (REST %d)",
                sym,
                missing,
                scheduled,
            )
        self._resync_state[sym] = meta
        ttl = int(WS_GAP_BACKFILL.get("status_ttl", 900))
        try:
            await self.store.redis.jset(
                *WS_GAP_STATUS_PATH,
                value=self._resync_state,
                ttl=ttl,
            )
        except Exception as e:
            logger.debug("[WS GAP] –ù–µ –≤–¥–∞–ª–æ—Å—è –æ–Ω–æ–≤–∏—Ç–∏ —Å—Ç–∞—Ç—É—Å resync: %s", e)

    async def _clear_resync(self, sym: str) -> None:
        if sym not in self._resync_state:
            return
        self._resync_state.pop(sym, None)
        ttl = int(WS_GAP_BACKFILL.get("status_ttl", 900))
        try:
            await self.store.redis.jset(
                *WS_GAP_STATUS_PATH,
                value=self._resync_state,
                ttl=ttl,
            )
            logger.info("[WS GAP] %s: —Ä–µ—Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞", sym)
        except Exception as e:
            logger.debug("[WS GAP] –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—á–∏—Å—Ç–∏—Ç–∏ —Å—Ç–∞—Ç—É—Å resync: %s", e)

    async def _resubscribe(self, ws: Any, new_syms: set[str]) -> None:
        """UNSUBSCRIBE/SUBSCRIBE WS-–∫–∞–Ω–∞–ª–∏ –±–µ–∑ reconnect."""
        old_syms = self._symbols
        to_unsub = [f"{s}@kline_1m" for s in old_syms - new_syms]
        to_sub = [f"{s}@kline_1m" for s in new_syms - old_syms]
        rid = 1
        if to_unsub:
            await ws.send(
                json.dumps({"method": "UNSUBSCRIBE", "params": to_unsub, "id": rid})
            )
            rid += 1
        if to_sub:
            await ws.send(
                json.dumps({"method": "SUBSCRIBE", "params": to_sub, "id": rid})
            )
            logger.debug(
                "WS re-subscribed +%d -%d total=%d",
                len(to_sub),
                len(to_unsub),
                len(new_syms),
            )
        self._symbols = new_syms

    async def consume(self) -> None:
        """–ì–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª: –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è, –æ–±—Ä–æ–±–∫–∞ WS, reconnect/backoff."""
        while not self._stop_event.is_set():
            try:
                # 1. –ó–∞–≤–∂–¥–∏ –≤–∏–∑–Ω–∞—á–∞—î–º–æ syms, fallback –Ω–∞ {"btcusdt"}
                syms = set(await self._get_live_symbols() or [])
                if not syms:
                    syms = {"btcusdt"}
                # 2. –û–Ω–æ–≤–ª—é—î–º–æ ws_url –ª–∏—à–µ —è–∫—â–æ —Å–∏–º–≤–æ–ª–∏ –∑–º—ñ–Ω–∏–ª–∏—Å—å
                if syms != self._symbols or not self._ws_url:
                    self._symbols = syms
                    self._ws_url = self._build_ws_url(self._symbols)

                logger.info(
                    "üîÑ –ó–∞–ø—É—Å–∫ WS (%d symbols): %s",
                    len(self._symbols),
                    list(self._symbols)[:5],
                )
                # –°—Ç–µ–Ω–æ–≥—Ä–∞–º–∞ –≤–∏–¥–∞–ª–µ–Ω–∞
                async with websockets.connect(self._ws_url, ping_interval=20) as ws:
                    logger.debug("WS connected (%d streams)‚Ä¶", len(self._symbols))
                    self._backoff = 3
                    self._stop_event.clear()
                    self._refresh_task = asyncio.create_task(self._refresh_symbols(ws))
                    # –°—Ç–µ–Ω–æ–≥—Ä–∞–º–∞ –≤–∏–¥–∞–ª–µ–Ω–∞
                    async for msg in ws:
                        try:
                            data = json.loads(msg).get("data", {}).get("k")
                            if data:
                                await self._handle_kline(data)
                        except Exception as e:
                            try:
                                if isinstance(msg, dict):
                                    _ = json.dumps(msg, default=str)[:80]
                                logger.debug(
                                    "Bad WS message: %s‚Ä¶ (%s)", str(msg)[:200], e
                                )
                            except Exception:
                                logger.debug("Bad WS message: <unserializable> (%s)", e)
            except Exception as exc:
                logger.warning("WS error: %s ‚Üí reconnect in %ds", exc, self._backoff)
                # –°—Ç–µ–Ω–æ–≥—Ä–∞–º–∞ –≤–∏–¥–∞–ª–µ–Ω–∞
                await asyncio.sleep(self._backoff)
                self._backoff = min(self._backoff * 2, 30)
            finally:
                if self._refresh_task:
                    self._refresh_task.cancel()
                if self._hb_task:
                    self._hb_task.cancel()

    async def stop(self) -> None:
        """–ó—É–ø–∏–Ω—è—î –≤–æ—Ä–∫–µ—Ä —ñ –≤—Å—ñ —Ñ–æ–Ω–æ–≤—ñ —Ç–∞—Å–∫–∏."""
        self._stop_event.set()
        if self._refresh_task:
            self._refresh_task.cancel()

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # –°—É–º—ñ—Å–Ω—ñ—Å—Ç—å —ñ–∑ —Ç–µ—Å—Ç–∞–º–∏: _reactive_stage1_call(mon, symbol, payload)
    # –Ø–∫—â–æ —É –º–æ–Ω—ñ—Ç–æ—Ä–∞ —î update_and_check ‚Üí –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ, —ñ–Ω–∞–∫—à–µ process_new_bar
    async def _reactive_stage1_call(
        self, monitor: Any, symbol: str, payload: Any
    ) -> None:
        # –Ø–∫—â–æ –Ω–µ–º–∞—î payload ‚Äî –æ–¥—Ä–∞–∑—É process_new_bar
        if payload is None:
            try:
                maybe2 = monitor.process_new_bar(symbol)
                if asyncio.iscoroutine(maybe2):
                    await maybe2
            except Exception:
                pass
            return
        # –Ü–Ω–∞–∫—à–µ ‚Äî —Å–ø—Ä–æ–±—É–≤–∞—Ç–∏ update_and_check, —è–∫—â–æ —î
        try:
            fn = getattr(monitor, "update_and_check", None)
            if callable(fn):
                maybe = fn(symbol, payload)
                if asyncio.iscoroutine(maybe):
                    await maybe
                return
        except Exception:
            pass
        # Fallback
        try:
            maybe2 = monitor.process_new_bar(symbol)
            if asyncio.iscoroutine(maybe2):
                await maybe2
        except Exception:
            pass


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ó–∞–ø—É—Å–∫ –º–æ–¥—É–ª—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

if __name__ == "__main__":
    # Minimal bootstrap for manual run: create Redis-less store if available
    try:
        from redis.asyncio import Redis as _Redis

        from data.unified_store import StoreConfig, StoreProfile, UnifiedDataStore

        # default ephemeral config
        _redis = _Redis(
            host=os.getenv("REDIS_HOST", "localhost"),
            port=int(os.getenv("REDIS_PORT", "6379")),
        )
        _cfg = StoreConfig(
            namespace=os.getenv("AI_ONE_NS", "ai_one"),
            base_dir=os.getcwd(),
            profile=StoreProfile(),
            intervals_ttl={"1m": 90, "1h": 65 * 60},
            write_behind=False,
            validate_on_read=False,
            validate_on_write=False,
            io_retry_attempts=2,
            io_retry_backoff=0.5,
        )
        _store = UnifiedDataStore(redis=_redis, cfg=_cfg)
    except Exception as _e:  # pragma: no cover
        logger.error("Failed to init UnifiedDataStore: %s", _e)
        raise

    worker = WSWorker(store=_store)
    try:
        asyncio.run(worker.consume())
    except KeyboardInterrupt:
        logger.info("WSWorker stopped by user")
    except Exception as e:
        logger.error("Fatal error: %s", e, exc_info=True)
