"""–®–≤–∏–¥–∫–∏–π —Ñ—ñ–ª—å—Ç—Ä USDT‚ÄëM Binance Futures —ñ–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏.

–®–ª—è—Ö: ``stage1/binance_future_asset_filter.py``

–ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ:
    ‚Ä¢ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π –∑–±—ñ—Ä (open interest / depth / ATR) –ø—ñ–¥ —Å–µ–º–∞—Ñ–æ—Ä–∞–º–∏;
    ‚Ä¢ –¥–∏–Ω–∞–º—ñ—á–Ω—ñ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å–Ω—ñ –ø–æ—Ä–æ–≥–∏ (quoteVolume / priceChangePercent);
    ‚Ä¢ –∫–µ—à—É–≤–∞–Ω–Ω—è exchangeInfo (Redis) + –ø–æ–≤—Ç–æ—Ä–Ω—ñ —Å–ø—Ä–æ–±–∏;
    ‚Ä¢ —Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è –∞–∫—Ç–∏–≤—ñ–≤ –∑–∞ –∫–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∏–º liquidity_score;
    ‚Ä¢ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—î—é (``stage1.visualization.print_results``).

–í–∏—Ö—ñ–¥: –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Ç—ñ–∫–µ—Ä—ñ–≤ –¥–ª—è –ø–æ–¥–∞–ª—å—à–∏—Ö —Å—Ç–∞–¥—ñ–π.
"""

import asyncio
import logging
import time

import aiohttp
import pandas as pd
from rich.console import Console
from rich.logging import RichHandler
from rich.progress import (
    BarColumn,
    MofNCompleteColumn,
    Progress,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
)

from config.config import (
    DEPTH_SEMAPHORE,
    KLINES_SEMAPHORE,
    OI_SEMAPHORE,
    STAGE1_METRICS_BATCH,
    STAGE1_PREFILTER_HEAVY_LIMIT,
    FilterParams,
    MetricResults,
    SymbolInfo,
)
from stage1.helpers import (
    _fetch_json,
    fetch_atr,
    fetch_cached_data,
    fetch_concurrently,
    fetch_open_interest,
    fetch_orderbook_depth,
)
from stage1.visualization import print_results
from utils.utils import format_open_interest, format_volume_usd

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –õ–æ–≥—É–≤–∞–Ω–Ω—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logger = logging.getLogger("app.stage1.binance_future_asset_filter")
if not logger.handlers:
    logger.setLevel(logging.INFO)
    logger.addHandler(RichHandler(console=Console(stderr=True), show_path=False))
    logger.propagate = False

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –∫–æ–Ω—Å–æ–ª—å –¥–ª—è –∑—Ä—É—á–Ω–æ—Å—Ç—ñ
console = Console()


WHITELIST_SYMBOLS: set[str] = {"XAUUSD"}


# CORE LOGIC
class BinanceFutureAssetFilter:
    # class-level –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É –¥–æ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –º–µ—Ç—Ä–∏–∫ —ñ–∑ –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö –º–æ–¥—É–ª—ñ–≤
    last_metrics: MetricResults | dict[str, object] | None = None

    def __init__(self, session: aiohttp.ClientSession, store):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ñ—ñ–ª—å—Ç—Ä–∞ –∞–∫—Ç–∏–≤—ñ–≤ Binance Futures.
        :param session: aiohttp.ClientSession –¥–ª—è HTTP-–∑–∞–ø–∏—Ç—ñ–≤
        :param store: UnifiedDataStore (—Ä–∞–Ω—ñ—à–µ –Ω–∞–∑–∏–≤–∞–ª–∏ cache_handler)
        """
        self.session = session
        self.store = store  # —ñ—Å—Ç–æ—Ä–∏—á–Ω–µ cache_handler ‚Üí —Ç–µ–ø–µ—Ä —á—ñ—Ç–∫—ñ—à–µ
        self.metrics: MetricResults | dict[str, object] = {}
        self.progress: Progress | None = None
        self.metrics_progress: Progress | None = (
            None  # –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –∞—Ç—Ä–∏–±—É—Ç –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—É –º–µ—Ç—Ä–∏–∫
        )
        logger.debug("BinanceFutureAssetFilter —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ")

    async def load_exchange_info(self) -> list[SymbolInfo]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ —Å–∏–º–≤–æ–ª–∏ –∑ –∫–µ—à—É –∞–±–æ API Binance.
        –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ SymbolInfo –¥–ª—è USDT-PERPETUAL TRADING —Å–∏–º–≤–æ–ª—ñ–≤.
        """
        logger.debug("[STEP] –ü–æ—á–∞—Ç–æ–∫ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è exchangeInfo")

        def process_data(data: dict | list) -> list[dict]:
            # –û–±—Ä–æ–±–∫–∞ —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤ –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
            logger.debug(f"[EVENT] –û–±—Ä–æ–±–∫–∞ exchangeInfo, —Ç–∏–ø: {type(data)}")
            symbols = data.get("symbols", []) if isinstance(data, dict) else data
            filtered = [
                s
                for s in symbols
                if s.get("quoteAsset") == "USDT"
                and s.get("status") == "TRADING"
                and s.get("contractType") == "PERPETUAL"
            ]
            logger.debug(
                f"[EVENT] –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(filtered)} —Å–∏–º–≤–æ–ª—ñ–≤ –∑ exchangeInfo"
            )
            return filtered

        data = await fetch_cached_data(
            self.session,
            self.store,
            "binance_futures_exchange_info",
            "https://fapi.binance.com/fapi/v1/exchangeInfo",
            process_data,
        )
        logger.debug(f"[STEP] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ exchangeInfo, –∫—ñ–ª—å–∫—ñ—Å—Ç—å: {len(data)}")
        # data expected as {"symbols": [...]} ; normalize to list
        items = data.get("symbols", []) if isinstance(data, dict) else data
        symbols = [SymbolInfo(**s) for s in items if isinstance(s, dict)]
        if len(symbols) <= 1:
            logger.debug(
                "[STEP] exchangeInfo cache –ø–æ–≤–µ—Ä–Ω—É–≤ –ª–∏—à–µ %d —Å–∏–º–≤–æ–ª—ñ–≤ ‚Äî —Ñ–æ—Ä—Å—É—î–º–æ refresh",
                len(symbols),
            )
            fresh = await _fetch_json(
                self.session, "https://fapi.binance.com/fapi/v1/exchangeInfo"
            )
            refreshed_items = process_data(fresh) if fresh else []
            symbols = [SymbolInfo(**s) for s in refreshed_items if isinstance(s, dict)]
            logger.debug(
                "[STEP] –§–æ—Ä—Å–æ–≤–∞–Ω–∏–π refresh exchangeInfo: %d —Å–∏–º–≤–æ–ª—ñ–≤",
                len(symbols),
            )
        return symbols

    async def fetch_ticker_data(self) -> pd.DataFrame:
        """
        –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö 24h ticker –∑ Binance Futures API.
        –ü–æ–≤–µ—Ä—Ç–∞—î DataFrame –∑ –¥–∞–Ω–∏–º–∏ –ø–æ –≤—Å—ñ—Ö —Å–∏–º–≤–æ–ª–∞—Ö.
        """
        url = "https://fapi.binance.com/fapi/v1/ticker/24hr"
        logger.debug(f"[STEP] –ó–∞–ø–∏—Ç ticker/24hr: {url}")
        try:
            data = await _fetch_json(self.session, url)
            logger.debug(f"[EVENT] –û—Ç—Ä–∏–º–∞–Ω–æ {len(data)} –∑–∞–ø–∏—Å—ñ–≤ ticker")
            return pd.DataFrame(data)
        except Exception as e:
            logger.error("–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è ticker –¥–∞–Ω–∏—Ö: %s", e)
            return pd.DataFrame()

    async def apply_dynamic_thresholds(
        self, df: pd.DataFrame, params: FilterParams
    ) -> FilterParams:
        """
        –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –¥–∏–Ω–∞–º—ñ—á–Ω–∏—Ö –ø–æ—Ä–æ–≥—ñ–≤ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –∞–∫—Ç–∏–≤—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—ñ–≤.
        –û–Ω–æ–≤–ª—é—î params –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ quoteVolume —Ç–∞ priceChangePercent.
        """
        logger.debug("[STEP] –ü–æ—á–∞—Ç–æ–∫ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É –¥–∏–Ω–∞–º—ñ—á–Ω–∏—Ö –ø–æ—Ä–æ–≥—ñ–≤")
        try:
            df = df.copy()
            df["quoteVolume"] = pd.to_numeric(df["quoteVolume"], errors="coerce")
            df["priceChangePercent"] = pd.to_numeric(
                df["priceChangePercent"], errors="coerce"
            ).abs()
            logger.debug(f"[EVENT] –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–æ —Ç–∏–ø–∏, –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤: {len(df)}")

            df = df.dropna(subset=["quoteVolume", "priceChangePercent"])
            logger.debug(f"[EVENT] –ü—ñ—Å–ª—è dropna: {len(df)} –∑–∞–ø–∏—Å—ñ–≤")

            if len(df) > 10:
                params.min_quote_volume = df["quoteVolume"].quantile(0.75)
                params.min_price_change = df["priceChangePercent"].quantile(0.70)
                logger.info(
                    "–î–∏–Ω–∞–º—ñ—á–Ω—ñ –ø–æ—Ä–æ–≥–∏: Vol ‚â• %.2f, Œî%% ‚â• %.2f",
                    params.min_quote_volume,
                    params.min_price_change,
                )
                logger.debug(
                    "[EVENT] –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ—Ä–æ–≥–∏: "
                    f"min_quote_volume={params.min_quote_volume}, "
                    f"min_price_change={params.min_price_change}"
                )
            else:
                logger.debug("[EVENT] –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è –¥–∏–Ω–∞–º—ñ—á–Ω–∏—Ö –ø–æ—Ä–æ–≥—ñ–≤")
            return params
        except Exception as e:
            logger.error("–ü–æ–º–∏–ª–∫–∞ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É –¥–∏–Ω–∞–º—ñ—á–Ω–∏—Ö –ø–æ—Ä–æ–≥—ñ–≤: %s", e)
            return params

    async def filter_assets(self, params: FilterParams) -> list[str]:
        """
        –û—Å–Ω–æ–≤–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –∞–∫—Ç–∏–≤—ñ–≤ Binance Futures.
        –í–∏–∫–æ–Ω—É—î –≤—Å—ñ –µ—Ç–∞–ø–∏: –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è, —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è, –∑–±—ñ—Ä –º–µ—Ç—Ä–∏–∫, —Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è.
        –ü–æ–≤–µ—Ä—Ç–∞—î –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª—ñ–≤.
        """
        start_time = time.monotonic()
        console.print("üîç [bold cyan]–ü–æ—á–∞—Ç–æ–∫ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –∞–∫—Ç–∏–≤—ñ–≤...[/bold cyan]")

        # –°—Ç–≤–æ—Ä—é—î–º–æ —î–¥–∏–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä –∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–º–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏
        self.progress = Progress(
            SpinnerColumn("dots", style="bold cyan"),
            BarColumn(
                bar_width=40,
                complete_style="bold rgb(0,200,0)",
                finished_style="bold green",
                pulse_style="bold yellow",
            ),
            TaskProgressColumn(
                text_format="[bold]{task.percentage:>3.0f}%[/bold]", style="bold white"
            ),
            TextColumn("‚Ä¢", style="dim"),
            MofNCompleteColumn(),
            TextColumn("‚Ä¢", style="dim"),
            TextColumn("[bold]{task.description}", style="bold white"),
            TextColumn("‚Ä¢", style="dim"),
            TimeElapsedColumn(),
            console=Console(stderr=True),
            transient=False,
            refresh_per_second=20,
        )
        self.progress.start()

        # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ—Ç–∞–ø—ñ–≤
        total_steps = 9
        main_task = self.progress.add_task("–§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∞–∫—Ç–∏–≤—ñ–≤...", total=total_steps)

        # –ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –±–∞–∑–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        logger.debug("[STEP] –ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è exchangeInfo")
        self.progress.update(main_task, description="[cyan]üîç –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö...")
        exchange_info = await self.load_exchange_info()
        valid_symbols = {s.symbol for s in exchange_info}
        logger.debug(f"[EVENT] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(valid_symbols)} –≤–∞–ª—ñ–¥–Ω–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤")

        self.progress.advance(main_task)

        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è ticker –¥–∞–Ω–∏—Ö
        self.progress.update(
            main_task, description="[cyan]üìä –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö —Ç–∏–∫–µ—Ä–∞..."
        )
        ticker_df = await self.fetch_ticker_data()
        logger.debug(f"[EVENT] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ ticker_df, shape: {ticker_df.shape}")
        if ticker_df.empty:
            logger.error("–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –¥–∞–Ω—ñ ticker")
            return []

        self.progress.advance(main_task)  # –ü—Ä–æ—Å—É–≤–∞—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å

        # –ö—Ä–æ–∫ 2: –î–∏–Ω–∞–º—ñ—á–Ω—ñ –ø–æ—Ä–æ–≥–∏ (—è–∫—â–æ –∞–∫—Ç–∏–≤–æ–≤–∞–Ω–æ)
        self.progress.update(main_task, description="[cyan]‚öôÔ∏è –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –ø–æ—Ä–æ–≥—ñ–≤...")
        logger.debug("[STEP] –ö—Ä–æ–∫ 2: –î–∏–Ω–∞–º—ñ—á–Ω—ñ –ø–æ—Ä–æ–≥–∏")
        if params.dynamic:
            params = await self.apply_dynamic_thresholds(ticker_df, params)
            logger.debug(f"[EVENT] –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –ø—ñ—Å–ª—è –¥–∏–Ω–∞–º—ñ–∫–∏: {params.dict()}")
        else:
            logger.debug("[EVENT] –î–∏–Ω–∞–º—ñ—á–Ω—ñ –ø–æ—Ä–æ–≥–∏ –≤–∏–º–∫–Ω–µ–Ω–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞—Ç–∏—á–Ω—ñ")

        self.progress.advance(main_task)

        # –ö—Ä–æ–∫ 3: –ë–∞–∑–æ–≤–∏–π —Ñ—ñ–ª—å—Ç—Ä
        logger.debug("[STEP] –ö—Ä–æ–∫ 3: –ë–∞–∑–æ–≤–∏–π —Ñ—ñ–ª—å—Ç—Ä")
        self.progress.update(main_task, description="[cyan]‚öôÔ∏è –ë–∞–∑–æ–≤–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è...")
        ticker_df = ticker_df[ticker_df["symbol"].isin(valid_symbols)].copy()
        ticker_df["quoteVolume"] = pd.to_numeric(
            ticker_df["quoteVolume"], errors="coerce"
        )
        ticker_df["priceChangePercent"] = pd.to_numeric(
            ticker_df["priceChangePercent"], errors="coerce"
        ).abs()
        logger.debug(f"[EVENT] –ü—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –ø–æ symbol: {ticker_df.shape}")

        base_mask = (ticker_df["quoteVolume"] >= params.min_quote_volume) & (
            ticker_df["priceChangePercent"] >= params.min_price_change
        )
        prefiltered_df = ticker_df[base_mask].copy()
        logger.debug(f"[EVENT] –ü—ñ—Å–ª—è –±–∞–∑–æ–≤–æ–≥–æ –º–∞—Å–∫—É: {prefiltered_df.shape}")

        missing_whitelist = [
            sym
            for sym in WHITELIST_SYMBOLS
            if sym in ticker_df["symbol"].values
            and sym not in prefiltered_df["symbol"].values
        ]
        if missing_whitelist:
            extra_rows = ticker_df[ticker_df["symbol"].isin(missing_whitelist)].copy()
            if not extra_rows.empty:
                prefiltered_df = (
                    pd.concat([prefiltered_df, extra_rows], ignore_index=True)
                    .drop_duplicates(subset="symbol")
                    .reset_index(drop=True)
                )
                logger.debug("[STEP] –î–æ–¥–∞–Ω–æ whitelist —Å–∏–º–≤–æ–ª–∏: %s", missing_whitelist)

        if prefiltered_df.empty:
            logger.warning("–ù–µ–º–∞—î –∞–∫—Ç–∏–≤—ñ–≤ –ø—ñ—Å–ª—è –±–∞–∑–æ–≤–æ—ó —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó")
            return []

        symbols = prefiltered_df["symbol"].tolist()
        logger.debug("–ü—ñ—Å–ª—è –±–∞–∑–æ–≤–æ—ó —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: %d –∞–∫—Ç–∏–≤—ñ–≤", len(symbols))

        heavy_limit = min(len(symbols), STAGE1_PREFILTER_HEAVY_LIMIT)
        if len(symbols) > heavy_limit:
            logger.debug(
                "[STEP] –û–±–º–µ–∂—É—î–º–æ –≤–∞–∂–∫—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ —Ç–æ–ø-%d —Å–∏–º–≤–æ–ª—ñ–≤ (–∑ %d)",
                heavy_limit,
                len(symbols),
            )
            prefiltered_df = (
                prefiltered_df.sort_values("quoteVolume", ascending=False)
                .head(heavy_limit)
                .copy()
            )
            symbols = prefiltered_df["symbol"].tolist()
            logger.debug("[STEP] –ü—ñ—Å–ª—è –æ–±—Ä—ñ–∑–∫–∏ –≤–∞–∂–∫–∏—Ö –º–µ—Ç—Ä–∏–∫: %d –∞–∫—Ç–∏–≤—ñ–≤", len(symbols))
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è —Å–∏–º–≤–æ–ª—ñ–≤ –¥–ª—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –º–µ—Ç—Ä–∏–∫
        logger.debug(f"[EVENT] –°–∏–º–≤–æ–ª–∏ –¥–ª—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –º–µ—Ç—Ä–∏–∫: {symbols}")

        self.progress.advance(main_task)

        # –ö—Ä–æ–∫ 4: –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π –∑–±—ñ—Ä –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –º–µ—Ç—Ä–∏–∫
        logger.debug("[STEP] –ö—Ä–æ–∫ 4: –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π –∑–±—ñ—Ä openInterest")
        self.progress.update(main_task, description="[cyan]üìà –ó–±—ñ—Ä –º–µ—Ç—Ä–∏–∫...")

        # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≤–¥–∞–Ω–Ω—è –¥–ª—è –º–µ—Ç—Ä–∏–∫ —Ç—ñ–ª—å–∫–∏ –∑–∞—Ä–∞–∑, –∫–æ–ª–∏ –∑–Ω–∞—î–º–æ symbols
        total_metrics = len(symbols) * 3
        assert self.progress is not None
        prog = self.progress
        metrics_task = prog.add_task(
            "[bold yellow] OI ‚Ä¢ Depth ‚Ä¢ ATR[/bold yellow]", total=total_metrics
        )

        batch_size = max(1, STAGE1_METRICS_BATCH)
        total = len(symbols)
        chunked = [
            symbols[idx : idx + batch_size] for idx in range(0, total, batch_size)
        ]

        def _log_batch(metric: str, batch_idx: int, chunk: list[str]) -> None:
            logger.debug(
                "[STEP] %s –±–∞—Ç—á %d/%d ‚Äî —Å–∏–º–≤–æ–ª—ñ–≤: %d",  # noqa: E501
                metric,
                batch_idx,
                max(1, len(chunked)),
                len(chunk),
            )

        oi_data: dict[str, float] = {}
        for idx, chunk in enumerate(chunked, start=1):
            _log_batch("openInterest", idx, chunk)
            partial = await fetch_concurrently(
                self.session,
                chunk,
                fetch_open_interest,
                OI_SEMAPHORE,
                progress_callback=lambda: prog.advance(metrics_task),
            )
            oi_data.update(partial)
        logger.debug("[EVENT] –ó—ñ–±—Ä–∞–Ω–æ openInterest –¥–ª—è %d —Å–∏–º–≤–æ–ª—ñ–≤", len(oi_data))

        logger.debug("[STEP] –ö—Ä–æ–∫ 4: –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π –∑–±—ñ—Ä orderbookDepth")

        depth_data: dict[str, float] = {}
        for idx, chunk in enumerate(chunked, start=1):
            _log_batch("orderbookDepth", idx, chunk)
            partial = await fetch_concurrently(
                self.session,
                chunk,
                fetch_orderbook_depth,
                DEPTH_SEMAPHORE,
                progress_callback=lambda: prog.advance(metrics_task),
            )
            depth_data.update(partial)
        logger.debug("[EVENT] –ó—ñ–±—Ä–∞–Ω–æ orderbookDepth –¥–ª—è %d —Å–∏–º–≤–æ–ª—ñ–≤", len(depth_data))

        logger.debug("[STEP] –ö—Ä–æ–∫ 4: –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π –∑–±—ñ—Ä ATR")

        atr_data: dict[str, float] = {}
        for idx, chunk in enumerate(chunked, start=1):
            _log_batch("ATR", idx, chunk)
            partial = await fetch_concurrently(
                self.session,
                chunk,
                fetch_atr,
                KLINES_SEMAPHORE,
                progress_callback=lambda: prog.advance(metrics_task),
            )
            atr_data.update(partial)
        logger.debug("[EVENT] –ó—ñ–±—Ä–∞–Ω–æ ATR –¥–ª—è %d —Å–∏–º–≤–æ–ª—ñ–≤", len(atr_data))

        self.progress.advance(main_task)

        # –ö—Ä–æ–∫ 5: –û–Ω–æ–≤–ª–µ–Ω–Ω—è DataFrame
        logger.debug("[STEP] –ö—Ä–æ–∫ 5: –û–Ω–æ–≤–ª–µ–Ω–Ω—è DataFrame –¥–æ–¥–∞—Ç–∫–æ–≤–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏")

        self.progress.update(main_task, description="–û–Ω–æ–≤–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö")
        self.progress.update(main_task, description="[cyan]üîÑ –û–Ω–æ–≤–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö...")
        prefiltered_df["openInterest"] = prefiltered_df["symbol"].map(oi_data)
        prefiltered_df["orderbookDepth"] = prefiltered_df["symbol"].map(depth_data)
        prefiltered_df["atrPercent"] = prefiltered_df["symbol"].map(atr_data)
        logger.debug(
            f"[EVENT] DataFrame –ø—ñ—Å–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –º–µ—Ç—Ä–∏–∫: {prefiltered_df.shape}"
        )
        self.progress.advance(main_task)

        # –õ–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –≥–ª–∏–±–∏–Ω–∏ —Å—Ç–∞–∫–∞–Ω—É —Ç–∞ open interest
        for row in prefiltered_df.itertuples(index=False):
            sym = str(getattr(row, "symbol", "") or "")
            depth_raw = getattr(row, "orderbookDepth", 0.0)
            oi_raw = getattr(row, "openInterest", 0.0)
            depth_val = float(depth_raw) if isinstance(depth_raw, (int, float)) else 0.0
            oi_val = float(oi_raw) if isinstance(oi_raw, (int, float)) else 0.0
            logger.debug(
                f"[EVENT] –ì–ª–∏–±–∏–Ω–∞ —Å—Ç–∞–∫–∞–Ω—É –¥–ª—è {sym}: {format_volume_usd(depth_val)}"
            )
            logger.debug(
                f"[EVENT] Open Interest –¥–ª—è {sym}: {format_open_interest(oi_val)}"
            )
        self.progress.update(main_task, advance=1)

        # –ö—Ä–æ–∫ 6: –î–æ–¥–∞—Ç–∫–æ–≤–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è
        logger.debug(
            "[STEP] –ö—Ä–æ–∫ 6: –î–æ–¥–∞—Ç–∫–æ–≤–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ openInterest, orderbookDepth, atrPercent"
        )
        self.progress.update(main_task, description="[cyan]‚öôÔ∏è –î–æ–¥–∞—Ç–∫–æ–≤–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è...")
        filtered_df = prefiltered_df[
            (prefiltered_df["openInterest"] >= params.min_open_interest)
            & (prefiltered_df["orderbookDepth"] >= params.min_orderbook_depth)
            & (prefiltered_df["atrPercent"] >= params.min_atr_percent)
        ].copy()
        logger.debug(f"[EVENT] –ü—ñ—Å–ª—è –¥–æ–¥–∞—Ç–∫–æ–≤–æ—ó —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: {filtered_df.shape}")

        if filtered_df.empty:
            logger.warning("–ù–µ–º–∞—î –∞–∫—Ç–∏–≤—ñ–≤ –ø—ñ—Å–ª—è –ø–æ–≤–Ω–æ—ó —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó")
            return []
        self.progress.advance(main_task)

        # –ö—Ä–æ–∫ 7: –†–∞–Ω–∂—É–≤–∞–Ω–Ω—è –∞–∫—Ç–∏–≤—ñ–≤
        logger.debug("[STEP] –ö—Ä–æ–∫ 7: –†–∞–Ω–∂—É–≤–∞–Ω–Ω—è –∞–∫—Ç–∏–≤—ñ–≤")
        self.progress.update(main_task, description="[cyan]üèÜ –†–∞–Ω–∂—É–≤–∞–Ω–Ω—è –∞–∫—Ç–∏–≤—ñ–≤...")
        filtered_df["liquidity_score"] = (
            0.5 * filtered_df["quoteVolume"] / filtered_df["quoteVolume"].max()
            + 0.3 * filtered_df["openInterest"] / filtered_df["openInterest"].max()
            + 0.2 * filtered_df["orderbookDepth"] / filtered_df["orderbookDepth"].max()
        )
        logger.debug(f"[EVENT] –î–æ–¥–∞–Ω–æ liquidity_score, shape: {filtered_df.shape}")

        result = (
            filtered_df.sort_values("liquidity_score", ascending=False)
            .head(params.max_symbols)["symbol"]
            .tolist()
        )
        logger.debug(f"[EVENT] –í—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–æ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —Å–ø–∏—Å–æ–∫: {result}")
        self.progress.advance(main_task)

        # –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è
        self.progress.update(
            main_task, description="[bold green]‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ![/bold green]"
        )

        # –ê–Ω—ñ–º–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è
        for _ in range(3):
            self.progress.update(
                main_task,
                description="[blink bold green]üöÄ –£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ![/blink bold green]",
            )
            await asyncio.sleep(0.2)
        self.progress.stop()

        # –ö—Ä–æ–∫ 8: –û–±—Ä–æ–±–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        elapsed = time.monotonic() - start_time

        # –°—Ç–≤–æ—Ä—é—î–º–æ –æ–±'—î–∫—Ç –º–µ—Ç—Ä–∏–∫
        metrics = MetricResults(
            initial_count=len(ticker_df),
            prefiltered_count=len(prefiltered_df),
            filtered_count=len(filtered_df),
            result_count=len(result),
            elapsed_time=elapsed,
            params=params.dict(),
        )

        # –í–∏–≤–æ–¥–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        print_results(result, metrics)

        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –Ω–∞–ª–∞–≥–æ–¥–∂–µ–Ω–Ω—è
        self.metrics = metrics
        # —Ç–∞–∫–æ–∂ —è–∫ class-level –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É –∑ —ñ–Ω—à–∏—Ö –º–æ–¥—É–ª—ñ–≤ (plain dict)
        try:
            type(self).last_metrics = metrics.dict()
        except Exception:
            type(self).last_metrics = {
                "initial_count": metrics.initial_count,
                "prefiltered_count": metrics.prefiltered_count,
                "filtered_count": metrics.filtered_count,
                "result_count": metrics.result_count,
                "elapsed_time": metrics.elapsed_time,
                "params": metrics.params,
            }
        logger.debug(f"[EVENT] –ó–±–µ—Ä–µ–∂–µ–Ω–æ –º–µ—Ç—Ä–∏–∫–∏: {self.metrics}")

        return result
